import cv2
import face_recognition
import os
import pickle
import json
from tqdm import tqdm
import pyttsx3


# Build Face Dataset
def build_face_dataset(dataset_folder="dataset", output_file="face_data.pkl"):
    known_encodings, known_names = [], []
    for filename in os.listdir(dataset_folder):
        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            path = os.path.join(dataset_folder, filename)
            image = face_recognition.load_image_file(path)
            encodings = face_recognition.face_encodings(image, model="cnn")
            if encodings:  # Only process images with detected faces
                name = filename.split("_")[0]  # Derive name from filename
                known_encodings.append(encodings[0])
                known_names.append(name)
    if known_encodings:  # Save only if there is valid data
        with open(output_file, "wb") as f:
            pickle.dump({"encodings": known_encodings, "names": known_names}, f)
        print(f"‚úÖ L∆∞u d·ªØ li·ªáu nh·∫≠n di·ªán v√†o {output_file}. T·ªïng: {len(known_names)} khu√¥n m·∫∑t.")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o ƒë·ªÉ hu·∫•n luy·ªán!")


# Detect Faces
def detect_faces(path, model_path="face_data.pkl", resize_scale=0.5, tolerance=0.6, display=True):
    try:
        with open(model_path, "rb") as f:
            data = pickle.load(f)
        known_encodings, known_names = data["encodings"], data["names"]
    except FileNotFoundError:
        print(f"‚ö†Ô∏è File {model_path} kh√¥ng t·ªìn t·∫°i! Vui l√≤ng t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.")
        return

    # Initialize Text-to-Speech Engine
    engine = pyttsx3.init()
    spoken = set()  # Track spoken names to avoid repetition
    is_image = path.lower().endswith(('.jpg', '.jpeg', '.png'))
    is_video = path.lower().endswith(('.mp4', '.avi', '.mov'))

    # Ensure output directories exist
    if not os.path.exists("output"):
        os.makedirs("output")
    os.makedirs("output/faces", exist_ok=True)

    metadata, total_faces = [], 0

    if is_image:
        img = cv2.imread(path)
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        small = cv2.resize(rgb, (0, 0), fx=resize_scale, fy=resize_scale)
        locs = face_recognition.face_locations(small, model="cnn")
        locs = [(int(t / resize_scale), int(r / resize_scale), int(b / resize_scale), int(l / resize_scale))
                for (t, r, b, l) in locs]
        encs = face_recognition.face_encodings(img, locs)

        for i, (enc, (top, right, bottom, left)) in enumerate(zip(encs, locs)):
            name = "Unknown"
            if known_encodings:
                distances = face_recognition.face_distance(known_encodings, enc)
                if distances.min() < tolerance:
                    idx = distances.argmin()
                    name = known_names[idx]
            if name != "Unknown" and name not in spoken:
                engine.say(f"Xin ch√†o {name}")
                engine.runAndWait()
                spoken.add(name)
            cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2)
            cv2.putText(img, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
            face_crop = rgb[top:bottom, left:right]
            cv2.imwrite(f"output/faces/face_{i}.jpg", cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))
            metadata.append({"name": name, "top": top, "right": right, "bottom": bottom, "left": left})
            total_faces += 1
        if display:
            cv2.imshow("Result", img)
            cv2.waitKey(0)
        cv2.imwrite("output/result_image.jpg", img)
        print(f"‚úÖ K·∫øt qu·∫£ l∆∞u t·∫°i: output/result_image.jpg. S·ªë khu√¥n m·∫∑t: {total_faces}")

    elif is_video:
        cap = cv2.VideoCapture(path)
        out = None
        pbar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc="ƒêang ph√¢n t√≠ch video")
        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        frame_id = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            small = cv2.resize(rgb, (0, 0), fx=resize_scale, fy=resize_scale)
            locs = face_recognition.face_locations(small, model="cnn")
            locs = [(int(t / resize_scale), int(r / resize_scale), int(b / resize_scale), int(l / resize_scale))
                    for (t, r, b, l) in locs]
            encs = face_recognition.face_encodings(frame, locs)

            for i, (enc, (top, right, bottom, left)) in enumerate(zip(encs, locs)):
                name = "Unknown"
                if known_encodings:
                    distances = face_recognition.face_distance(known_encodings, enc)
                    if distances.min() < tolerance:
                        idx = distances.argmin()
                        name = known_names[idx]
                if name != "Unknown" and name not in spoken:
                    engine.say(f"{name}")
                    engine.runAndWait()
                    spoken.add(name)
                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
                cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
                metadata.append({"frame": frame_id, "name": name, "top": top, "right": right, "bottom": bottom,
                                 "left": left})
                total_faces += 1

            if out is None:
                h, w = frame.shape[:2]
                out = cv2.VideoWriter("output/result_video.avi", fourcc, 20.0, (w, h))
            out.write(frame)
            if display and frame_id % 10 == 0:
                cv2.imshow("Video", frame)
                cv2.waitKey(1)
            frame_id += 1
            pbar.update(1)
        pbar.close()
        cap.release()
        if out:
            out.release()
        print("‚úÖ Video ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: output/result_video.avi")

    # Save metadata
    with open("output/metadata.json", "w", encoding="utf-8") as f:
        json.dump({"file": path, "faces": metadata, "total": total_faces}, f, indent=2)
    print(f"‚úÖ Ph√°t hi·ªán {total_faces} khu√¥n m·∫∑t. L∆∞u metadata v√†o output/metadata.json")


# Main Function
def main():
    print("üöÄ B·∫Øt ƒë·∫ßu ch∆∞∆°ng tr√¨nh nh·∫≠n di·ªán khu√¥n m·∫∑t...")
    dataset_folder = r"C:\Users\BHXH\Desktop\venv_demo\huanLuyen"  # Training data
    test_path = r"C:\Users\BHXH\Desktop\venv_demo\anh1.jpeg"  # Test file

    build_face_dataset(dataset_folder=dataset_folder, output_file="face_data.pkl")
    detect_faces(path=test_path, model_path="face_data.pkl", display=True)


if __name__ == "__main__":
    main()